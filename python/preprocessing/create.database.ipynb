{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "318337fb",
   "metadata": {},
   "source": [
    "# `Create Database for MIMIC Dataset`\n",
    "\n",
    "Creates SQLite dataset for MIMIC-IV v3.1 \n",
    "\n",
    "### Access Data Files\n",
    "- [ ] file1\n",
    "- [ ] file2\n",
    "\n",
    "### Create Databases\n",
    "- [ ] table1\n",
    "- [ ] table2\n",
    "\n",
    "### Read Aquisition Parameters\n",
    "- [ ] table1\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7dbc5f4",
   "metadata": {},
   "source": [
    "## 1. `Define Data Files`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3bc1dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add project root to sys.path\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "ROOT = Path.cwd()\n",
    "for parent in [ROOT] + list(ROOT.parents):\n",
    "    if (parent / \"config\").is_dir():\n",
    "        ROOT = parent\n",
    "        break\n",
    "if str(ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(ROOT))\n",
    "\n",
    "print( '[X] Root fount' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a6ef4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read settings and verify CSV files exist\n",
    "\n",
    "from xrh.settings import load_settings\n",
    "\n",
    "\n",
    "settings = load_settings()\n",
    "if not settings.admission_csv.exists():\n",
    "    raise FileNotFoundError(f\"Admission CSV file not found at {settings.admission_csv}\")\n",
    "\n",
    "if not settings.patient_csv.exists():\n",
    "    raise FileNotFoundError(f\"Patient CSV file not found at {settings.patient_csv}\")\n",
    "\n",
    "if not settings.transfer_csv.exists():\n",
    "    raise FileNotFoundError(f\"Transfer CSV file not found at {settings.transfer_csv}\")\n",
    "\n",
    "if not settings.d_hcpcs_csv.exists():\n",
    "    raise FileNotFoundError(f\"D HCPCS CSV file not found at {settings.d_hcpcs_csv}\")\n",
    "\n",
    "if not settings.d_icd_diagnoses_csv.exists():\n",
    "    raise FileNotFoundError(f\"D ICD Diagnoses CSV file not found at {settings.d_icd_diagnoses_csv}\")\n",
    "\n",
    "if not settings.d_icd_procedures_csv.exists():\n",
    "    raise FileNotFoundError(f\"D ICD Procedures CSV file not found at {settings.d_icd_procedures_csv}\")\n",
    "\n",
    "if not settings.d_labitems_csv.exists():\n",
    "    raise FileNotFoundError(f\"D Lab Items CSV file not found at {settings.d_labitems_csv}\")\n",
    "\n",
    "if not settings.diagnoses_icd_csv.exists():\n",
    "    raise FileNotFoundError(f\"Diagnoses ICD CSV file not found at {settings.diagnoses_icd_csv}\")\n",
    "\n",
    "if not settings.drgcodes_csv.exists():\n",
    "    raise FileNotFoundError(f\"DRG Codes CSV file not found at {settings.drgcodes_csv}\")\n",
    "\n",
    "if not settings.emar_csv.exists():\n",
    "    raise FileNotFoundError(f\"EMAR CSV file not found at {settings.emar_csv}\")\n",
    "\n",
    "if not settings.emar_detail_csv.exists():\n",
    "    raise FileNotFoundError(f\"EMAR Detail CSV file not found at {settings.emar_detail_csv}\")\n",
    "\n",
    "if not settings.hcpcsevents_csv.exists():\n",
    "    raise FileNotFoundError(f\"HCPCS Events CSV file not found at {settings.hcpcsevents_csv}\")\n",
    "\n",
    "if not settings.labevents_csv.exists():\n",
    "    raise FileNotFoundError(f\"Lab Events CSV file not found at {settings.labevents_csv}\")\n",
    "\n",
    "if not settings.microbiologyevents_csv.exists():\n",
    "    raise FileNotFoundError(f\"Microbiology Events CSV file not found at {settings.microbiologyevents_csv}\")\n",
    "\n",
    "if not settings.pharmacy_csv.exists():\n",
    "    raise FileNotFoundError(f\"Pharmacy CSV file not found at {settings.pharmacy_csv}\")\n",
    "\n",
    "if not settings.poe_csv.exists():\n",
    "    raise FileNotFoundError(f\"POE CSV file not found at {settings.poe_csv}\")\n",
    "\n",
    "if not settings.poe_detail_csv.exists():\n",
    "    raise FileNotFoundError(f\"POE Detail CSV file not found at {settings.poe_detail_csv}\")\n",
    "\n",
    "if not settings.prescriptions_csv.exists():\n",
    "    raise FileNotFoundError(f\"Prescriptions CSV file not found at {settings.prescriptions_csv}\")\n",
    "\n",
    "if not settings.procedures_icd_csv.exists():\n",
    "    raise FileNotFoundError(f\"Procedures ICD CSV file not found at {settings.procedures_icd_csv}\")\n",
    "\n",
    "if not settings.services_csv.exists():\n",
    "    raise FileNotFoundError(f\"Services CSV file not found at {settings.services_csv}\")\n",
    "\n",
    "print( '[X] All required CSV files found' )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f49deeee",
   "metadata": {},
   "source": [
    "# 2. `Create Databese`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40c675dc",
   "metadata": {},
   "source": [
    "### 2.1.01 `Hosp: Admission`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d86102",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "\n",
    "# === CONFIG: set your paths here ===\n",
    "db_dir   = settings.DB_path.parent                         # folder where the DB will live\n",
    "csv_path = settings.admission_csv       # <-- change to your CSV path\n",
    "\n",
    "db_dir.mkdir(parents=True, exist_ok=True)\n",
    "db_path = settings.DB_path            # database filename\n",
    "\n",
    "# === 1) Create the database and the admissions table ===\n",
    "schema_sql = \"\"\"\n",
    "DROP TABLE IF EXISTS admissions;\n",
    "CREATE TABLE admissions (\n",
    "    subject_id             INTEGER NOT NULL,\n",
    "    hadm_id                INTEGER NOT NULL,\n",
    "    admittime              TEXT    NOT NULL,   -- store TIMESTAMP as ISO-8601 text\n",
    "    dischtime              TEXT,\n",
    "    deathtime              TEXT,\n",
    "    admission_type         TEXT    NOT NULL,\n",
    "    admit_provider_id      TEXT,\n",
    "    admission_location     TEXT,\n",
    "    discharge_location     TEXT,\n",
    "    insurance              TEXT,\n",
    "    language               TEXT,\n",
    "    marital_status         TEXT,\n",
    "    race                   TEXT,\n",
    "    edregtime              TEXT,\n",
    "    edouttime              TEXT,\n",
    "    hospital_expire_flag   INTEGER\n",
    ");\n",
    "-- Optional helpful indexes (uncomment if you want them)\n",
    "-- CREATE INDEX idx_admissions_hadm ON admissions(hadm_id);\n",
    "-- CREATE INDEX idx_admissions_subject ON admissions(subject_id);\n",
    "\"\"\"\n",
    "\n",
    "conn = sqlite3.connect(db_path)\n",
    "with conn:\n",
    "    conn.executescript(schema_sql)\n",
    "\n",
    "print(f\"✅ Created database at: {db_path.resolve()}\")\n",
    "print(\"✅ Created table: admissions\")\n",
    "\n",
    "# === 2) Load CSV and insert into admissions ===\n",
    "# If your CSV columns already match exactly, this will just work.\n",
    "# Otherwise, you can rename columns via the 'rename_map' below.\n",
    "parse_date_cols = [\n",
    "    \"admittime\", \"dischtime\", \"deathtime\", \"edregtime\", \"edouttime\"\n",
    "]\n",
    "\n",
    "# Read CSV; if your timestamps are ISO-8601 already, you can set dtype=str instead of parse_dates\n",
    "try:\n",
    "    df = pd.read_csv(csv_path, parse_dates=parse_date_cols, infer_datetime_format=True, keep_date_col=True)\n",
    "except ValueError:\n",
    "    # Fall back if some timestamp cols are missing in the CSV\n",
    "    df = pd.read_csv(csv_path)\n",
    "\n",
    "# Ensure column names match the target table\n",
    "expected_cols = [\n",
    "    \"subject_id\",\"hadm_id\",\"admittime\",\"dischtime\",\"deathtime\",\n",
    "    \"admission_type\",\"admit_provider_id\",\"admission_location\",\"discharge_location\",\n",
    "    \"insurance\",\"language\",\"marital_status\",\"race\",\"edregtime\",\"edouttime\",\n",
    "    \"hospital_expire_flag\"\n",
    "]\n",
    "\n",
    "# If your CSV headers differ, map them here, e.g. {'SUBJECT_ID':'subject_id', 'HADM_ID':'hadm_id', ...}\n",
    "rename_map = {}\n",
    "if rename_map:\n",
    "    df = df.rename(columns=rename_map)\n",
    "\n",
    "# Keep only expected columns (and in the right order)\n",
    "missing = [c for c in expected_cols if c not in df.columns]\n",
    "if missing:\n",
    "    raise ValueError(f\"The CSV is missing required columns: {missing}\")\n",
    "\n",
    "df = df[expected_cols]\n",
    "\n",
    "# Convert datetime columns to ISO 8601 strings for SQLite (TEXT)\n",
    "for c in [\"admittime\",\"dischtime\",\"deathtime\",\"edregtime\",\"edouttime\"]:\n",
    "    if c in df.columns:\n",
    "        # Convert to string only where not null\n",
    "        if pd.api.types.is_datetime64_any_dtype(df[c]):\n",
    "            df[c] = df[c].dt.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        else:\n",
    "            # If already strings, leave as-is\n",
    "            pass\n",
    "\n",
    "# Enforce integer types where appropriate (SQLite is flexible, but this helps cleanliness)\n",
    "int_cols = [\"subject_id\",\"hadm_id\",\"hospital_expire_flag\"]\n",
    "for c in int_cols:\n",
    "    if c in df.columns:\n",
    "        df[c] = pd.to_numeric(df[c], errors=\"coerce\").astype(\"Int64\")\n",
    "\n",
    "# Insert into the table\n",
    "# We defined the table explicitly, so we APPEND.\n",
    "with conn:\n",
    "    df.to_sql(\"admissions\", conn, if_exists=\"append\", index=False)\n",
    "\n",
    "# Quick verification\n",
    "row_count = conn.execute(\"SELECT COUNT(*) FROM admissions\").fetchone()[0]\n",
    "print(f\"✅ Inserted {row_count} rows into admissions\")\n",
    "\n",
    "# Peek at a few rows\n",
    "preview = pd.read_sql(\"SELECT * FROM admissions LIMIT 5;\", conn)\n",
    "conn.close()\n",
    "preview\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f97654a",
   "metadata": {},
   "source": [
    "### 2.1.02 `Hosp: OMR`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a2d70c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CREATE TABLE: omr ===\n",
    "schema_sql_omr = \"\"\"\n",
    "DROP TABLE IF EXISTS omr;\n",
    "CREATE TABLE omr (\n",
    "    subject_id    INTEGER NOT NULL,\n",
    "    chartdate     TEXT    NOT NULL,   -- store DATE as ISO-8601 'YYYY-MM-DD'\n",
    "    seq_num       INTEGER NOT NULL,\n",
    "    result_name   TEXT    NOT NULL,\n",
    "    result_value  TEXT    NOT NULL\n",
    ");\n",
    "/* Optional indexes (uncomment if useful)\n",
    "CREATE INDEX IF NOT EXISTS idx_omr_subject ON omr(subject_id);\n",
    "CREATE INDEX IF NOT EXISTS idx_omr_subject_date ON omr(subject_id, chartdate);\n",
    "*/\n",
    "\"\"\"\n",
    "\n",
    "with conn:\n",
    "    conn.executescript(schema_sql_omr)\n",
    "print(\"✅ Table 'omr' created.\")\n",
    "\n",
    "# === LOAD CSV -> omr ===\n",
    "# Adjust this path if your settings object uses a different attribute name\n",
    "omr_csv_path = getattr(settings, \"omr_csv\", None) or csv_path  # fallback to previous csv_path if needed\n",
    "\n",
    "# Columns expected in the CSV\n",
    "expected_cols = [\"subject_id\", \"chartdate\", \"seq_num\", \"result_name\", \"result_value\"]\n",
    "\n",
    "# Read CSV; parse chartdate as date if present\n",
    "try:\n",
    "    df_omr = pd.read_csv(omr_csv_path, parse_dates=[\"chartdate\"], infer_datetime_format=True, keep_date_col=True)\n",
    "except ValueError:\n",
    "    # If 'chartdate' not present or parse error, read without date parsing\n",
    "    df_omr = pd.read_csv(omr_csv_path)\n",
    "\n",
    "# If your CSV headers differ, map them here (example shown; edit/remove as needed)\n",
    "# rename_map = {\"SUBJECT_ID\": \"subject_id\", \"CHARTDATE\": \"chartdate\", \"SEQ_NUM\": \"seq_num\",\n",
    "#               \"RESULT_NAME\": \"result_name\", \"RESULT_VALUE\": \"result_value\"}\n",
    "rename_map = {}\n",
    "if rename_map:\n",
    "    df_omr = df_omr.rename(columns=rename_map)\n",
    "\n",
    "# Validate required columns, enforce order\n",
    "missing = [c for c in expected_cols if c not in df_omr.columns]\n",
    "if missing:\n",
    "    raise ValueError(f\"OMR CSV is missing required columns: {missing}\")\n",
    "\n",
    "df_omr = df_omr[expected_cols]\n",
    "\n",
    "# Format chartdate to 'YYYY-MM-DD' strings (SQLite stores as TEXT)\n",
    "if \"chartdate\" in df_omr.columns:\n",
    "    if pd.api.types.is_datetime64_any_dtype(df_omr[\"chartdate\"]):\n",
    "        df_omr[\"chartdate\"] = df_omr[\"chartdate\"].dt.strftime(\"%Y-%m-%d\")\n",
    "    else:\n",
    "        # If it's already a string, normalize by slicing the date part (optional)\n",
    "        df_omr[\"chartdate\"] = df_omr[\"chartdate\"].astype(str).str[:10]\n",
    "\n",
    "# Clean integer columns\n",
    "for c in [\"subject_id\", \"seq_num\"]:\n",
    "    if c in df_omr.columns:\n",
    "        df_omr[c] = pd.to_numeric(df_omr[c], errors=\"coerce\").astype(\"Int64\")\n",
    "\n",
    "# Insert into SQLite\n",
    "with conn:\n",
    "    df_omr.to_sql(\"omr\", conn, if_exists=\"append\", index=False)\n",
    "\n",
    "# Quick verification\n",
    "rows = conn.execute(\"SELECT COUNT(*) FROM omr;\").fetchone()[0]\n",
    "print(f\"✅ Loaded {rows} rows into 'omr'.\")\n",
    "display(pd.read_sql(\"SELECT * FROM omr LIMIT 5;\", conn))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ba4dbd",
   "metadata": {},
   "source": [
    "### 2.1.03 `Hosp: Provider`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dffdc89c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CREATE TABLE: provider ===\n",
    "schema_sql_provider = \"\"\"\n",
    "DROP TABLE IF EXISTS provider;\n",
    "CREATE TABLE provider (\n",
    "    provider_id TEXT NOT NULL\n",
    ");\n",
    "/* Optional index if provider_id is often used for joins */\n",
    "-- CREATE UNIQUE INDEX IF NOT EXISTS idx_provider_id ON provider(provider_id);\n",
    "\"\"\"\n",
    "\n",
    "with conn:\n",
    "    conn.executescript(schema_sql_provider)\n",
    "print(\"✅ Table 'provider' created.\")\n",
    "+\n",
    "# === LOAD CSV -> provider ===\n",
    "# Adjust CSV path as needed; expects settings.provider_csv to exist\n",
    "provider_csv_path = getattr(settings, \"provider_csv\", None)\n",
    "\n",
    "if provider_csv_path is None:\n",
    "    raise ValueError(\"⚠️ Please define settings.provider_csv with the path to your provider CSV file.\")\n",
    "\n",
    "# Read CSV\n",
    "df_provider = pd.read_csv(provider_csv_path, dtype=str)  # ensure provider_id stays as text\n",
    "\n",
    "# Normalize column name (handle variations like 'PROVIDER_ID')\n",
    "rename_map = {\"PROVIDER_ID\": \"provider_id\"}\n",
    "df_provider = df_provider.rename(columns={c: c.lower() for c in df_provider.columns})\n",
    "df_provider = df_provider.rename(columns=rename_map)\n",
    "\n",
    "# Validate expected column\n",
    "if \"provider_id\" not in df_provider.columns:\n",
    "    raise ValueError(\"⚠️ The provider CSV must contain a 'provider_id' column.\")\n",
    "\n",
    "# Keep only that column (avoid extra)\n",
    "df_provider = df_provider[[\"provider_id\"]]\n",
    "\n",
    "# Drop duplicates if any\n",
    "df_provider = df_provider.drop_duplicates()\n",
    "\n",
    "# Insert into SQLite\n",
    "with conn:\n",
    "    df_provider.to_sql(\"provider\", conn, if_exists=\"append\", index=False)\n",
    "\n",
    "# Quick verification\n",
    "count = conn.execute(\"SELECT COUNT(*) FROM provider;\").fetchone()[0]\n",
    "print(f\"✅ Loaded {count} rows into 'provider'.\")\n",
    "display(pd.read_sql(\"SELECT * FROM provider LIMIT 5;\", conn))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
