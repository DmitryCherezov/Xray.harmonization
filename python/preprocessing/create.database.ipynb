{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "318337fb",
   "metadata": {},
   "source": [
    "# `Create Database for MIMIC Dataset`\n",
    "\n",
    "Creates SQLite dataset for MIMIC-IV v3.1 \n",
    "\n",
    "### Access Data Files\n",
    "- [ ] file1\n",
    "- [ ] file2\n",
    "\n",
    "### Create Databases\n",
    "- [ ] table1\n",
    "- [ ] table2\n",
    "\n",
    "### Read Aquisition Parameters\n",
    "- [ ] table1\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7dbc5f4",
   "metadata": {},
   "source": [
    "## 1. `Define Data Files`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3bc1dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add project root to sys.path\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "ROOT = Path.cwd()\n",
    "for parent in [ROOT] + list(ROOT.parents):\n",
    "    if (parent / \"config\").is_dir():\n",
    "        ROOT = parent\n",
    "        break\n",
    "if str(ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(ROOT))\n",
    "\n",
    "print( '[X] Root fount' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a6ef4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read settings and verify CSV files exist\n",
    "\n",
    "from xrh.settings import load_settings\n",
    "\n",
    "\n",
    "settings = load_settings()\n",
    "if not settings.admission_csv.exists():\n",
    "    raise FileNotFoundError(f\"Admission CSV file not found at {settings.admission_csv}\")\n",
    "\n",
    "if not settings.patient_csv.exists():\n",
    "    raise FileNotFoundError(f\"Patient CSV file not found at {settings.patient_csv}\")\n",
    "\n",
    "if not settings.transfer_csv.exists():\n",
    "    raise FileNotFoundError(f\"Transfer CSV file not found at {settings.transfer_csv}\")\n",
    "\n",
    "if not settings.d_hcpcs_csv.exists():\n",
    "    raise FileNotFoundError(f\"D HCPCS CSV file not found at {settings.d_hcpcs_csv}\")\n",
    "\n",
    "if not settings.d_icd_diagnoses_csv.exists():\n",
    "    raise FileNotFoundError(f\"D ICD Diagnoses CSV file not found at {settings.d_icd_diagnoses_csv}\")\n",
    "\n",
    "if not settings.d_icd_procedures_csv.exists():\n",
    "    raise FileNotFoundError(f\"D ICD Procedures CSV file not found at {settings.d_icd_procedures_csv}\")\n",
    "\n",
    "if not settings.d_labitems_csv.exists():\n",
    "    raise FileNotFoundError(f\"D Lab Items CSV file not found at {settings.d_labitems_csv}\")\n",
    "\n",
    "if not settings.diagnoses_icd_csv.exists():\n",
    "    raise FileNotFoundError(f\"Diagnoses ICD CSV file not found at {settings.diagnoses_icd_csv}\")\n",
    "\n",
    "if not settings.drgcodes_csv.exists():\n",
    "    raise FileNotFoundError(f\"DRG Codes CSV file not found at {settings.drgcodes_csv}\")\n",
    "\n",
    "if not settings.emar_csv.exists():\n",
    "    raise FileNotFoundError(f\"EMAR CSV file not found at {settings.emar_csv}\")\n",
    "\n",
    "if not settings.emar_detail_csv.exists():\n",
    "    raise FileNotFoundError(f\"EMAR Detail CSV file not found at {settings.emar_detail_csv}\")\n",
    "\n",
    "if not settings.hcpcsevents_csv.exists():\n",
    "    raise FileNotFoundError(f\"HCPCS Events CSV file not found at {settings.hcpcsevents_csv}\")\n",
    "\n",
    "if not settings.labevents_csv.exists():\n",
    "    raise FileNotFoundError(f\"Lab Events CSV file not found at {settings.labevents_csv}\")\n",
    "\n",
    "if not settings.microbiologyevents_csv.exists():\n",
    "    raise FileNotFoundError(f\"Microbiology Events CSV file not found at {settings.microbiologyevents_csv}\")\n",
    "\n",
    "if not settings.pharmacy_csv.exists():\n",
    "    raise FileNotFoundError(f\"Pharmacy CSV file not found at {settings.pharmacy_csv}\")\n",
    "\n",
    "if not settings.poe_csv.exists():\n",
    "    raise FileNotFoundError(f\"POE CSV file not found at {settings.poe_csv}\")\n",
    "\n",
    "if not settings.poe_detail_csv.exists():\n",
    "    raise FileNotFoundError(f\"POE Detail CSV file not found at {settings.poe_detail_csv}\")\n",
    "\n",
    "if not settings.prescriptions_csv.exists():\n",
    "    raise FileNotFoundError(f\"Prescriptions CSV file not found at {settings.prescriptions_csv}\")\n",
    "\n",
    "if not settings.procedures_icd_csv.exists():\n",
    "    raise FileNotFoundError(f\"Procedures ICD CSV file not found at {settings.procedures_icd_csv}\")\n",
    "\n",
    "if not settings.services_csv.exists():\n",
    "    raise FileNotFoundError(f\"Services CSV file not found at {settings.services_csv}\")\n",
    "\n",
    "print( '[X] All required CSV files found' )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f49deeee",
   "metadata": {},
   "source": [
    "# 2.1 `Create Databese: HOSP module`\n",
    "\n",
    "\n",
    "\n",
    "| **Done** | **Table Name**        | **Description** |\n",
    "|-----------|-----------------------|-----------------|\n",
    "| [X] | **omr**                | The Online Medical Record (OMR) table contains miscellaneous information from the EHR. |\n",
    "| [X] | **provider**           | The provider table lists deidentified provider identifiers used in the database. |\n",
    "| [X] | **admissions**         | Detailed information about hospital stays. |\n",
    "| [ ] | **d_hcpcs**            | Dimension table for hcpcsevents; provides a description of CPT codes. |\n",
    "| [ ] | **d_icd_diagnoses**    | Dimension table for diagnoses_icd; provides a description of ICD-9/ICD-10 billed diagnoses. |\n",
    "| [ ] | **d_icd_procedures**   | Dimension table for procedures_icd; provides a description of ICD-9/ICD-10 billed procedures. |\n",
    "| [ ] | **d_labitems**         | Dimension table for labevents; provides a description of all lab items. |\n",
    "| [ ] | **diagnoses_icd**      | Billed ICD-9/ICD-10 diagnoses for hospitalizations. |\n",
    "| [ ] | **drgcodes**           | Billed diagnosis related group (DRG) codes for hospitalizations. |\n",
    "| [ ] | **emar**               | The Electronic Medicine Administration Record (eMAR); barcode scanning of medications at the time of administration. |\n",
    "| [ ] | **emar_detail**        | Supplementary information for electronic administrations recorded in emar. |\n",
    "| [ ] | **hcpcsevents**        | Billed events occurring during the hospitalization. Includes CPT codes. |\n",
    "| [ ] | **labevents**          | Laboratory measurements sourced from patient derived specimens. |\n",
    "| [ ] | **microbiologyevents** | Microbiology cultures. |\n",
    "| [ ] | **patients**           | Patients' gender, age, and date of death if information exists. |\n",
    "| [ ] | **pharmacy**           | Formulary, dosing, and other information for prescribed medications. |\n",
    "| [ ] | **poe**                | Orders made by providers relating to patient care. |\n",
    "| [ ] | **poe_detail**         | Supplementary information for orders made by providers in the hospital. |\n",
    "| [ ] | **prescriptions**      | Prescribed medications. |\n",
    "| [ ] | **procedures_icd**     | Billed procedures for patients during their hospital stay. |\n",
    "| [ ] | **services**           | The hospital service(s) which cared for the patient during their hospitalization. |\n",
    "| [ ] | **transfers**          | Detailed information about patients' unit transfers. |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f97654a",
   "metadata": {},
   "source": [
    "### 2.1.01 `Hosp: OMR`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ef7e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CREATE TABLE: omr ===\n",
    "schema_sql_omr = \"\"\"\n",
    "DROP TABLE IF EXISTS omr;\n",
    "CREATE TABLE omr (\n",
    "    subject_id    INTEGER NOT NULL,\n",
    "    chartdate     TEXT    NOT NULL,   -- store DATE as ISO-8601 'YYYY-MM-DD'\n",
    "    seq_num       INTEGER NOT NULL,\n",
    "    result_name   TEXT    NOT NULL,\n",
    "    result_value  TEXT    NOT NULL\n",
    ");\n",
    "/* Optional indexes (uncomment if useful)\n",
    "CREATE INDEX IF NOT EXISTS idx_omr_subject ON omr(subject_id);\n",
    "CREATE INDEX IF NOT EXISTS idx_omr_subject_date ON omr(subject_id, chartdate);\n",
    "*/\n",
    "\"\"\"\n",
    "\n",
    "with conn:\n",
    "    conn.executescript(schema_sql_omr)\n",
    "print(\"✅ Table 'omr' created.\")\n",
    "\n",
    "# === LOAD CSV -> omr ===\n",
    "# Adjust this path if your settings object uses a different attribute name\n",
    "omr_csv_path = getattr(settings, \"omr_csv\", None) or csv_path  # fallback to previous csv_path if needed\n",
    "\n",
    "# Columns expected in the CSV\n",
    "expected_cols = [\"subject_id\", \"chartdate\", \"seq_num\", \"result_name\", \"result_value\"]\n",
    "\n",
    "# Read CSV; parse chartdate as date if present\n",
    "try:\n",
    "    df_omr = pd.read_csv(omr_csv_path, parse_dates=[\"chartdate\"], infer_datetime_format=True, keep_date_col=True)\n",
    "except ValueError:\n",
    "    # If 'chartdate' not present or parse error, read without date parsing\n",
    "    df_omr = pd.read_csv(omr_csv_path)\n",
    "\n",
    "# If your CSV headers differ, map them here (example shown; edit/remove as needed)\n",
    "# rename_map = {\"SUBJECT_ID\": \"subject_id\", \"CHARTDATE\": \"chartdate\", \"SEQ_NUM\": \"seq_num\",\n",
    "#               \"RESULT_NAME\": \"result_name\", \"RESULT_VALUE\": \"result_value\"}\n",
    "rename_map = {}\n",
    "if rename_map:\n",
    "    df_omr = df_omr.rename(columns=rename_map)\n",
    "\n",
    "# Validate required columns, enforce order\n",
    "missing = [c for c in expected_cols if c not in df_omr.columns]\n",
    "if missing:\n",
    "    raise ValueError(f\"OMR CSV is missing required columns: {missing}\")\n",
    "\n",
    "df_omr = df_omr[expected_cols]\n",
    "\n",
    "# Format chartdate to 'YYYY-MM-DD' strings (SQLite stores as TEXT)\n",
    "if \"chartdate\" in df_omr.columns:\n",
    "    if pd.api.types.is_datetime64_any_dtype(df_omr[\"chartdate\"]):\n",
    "        df_omr[\"chartdate\"] = df_omr[\"chartdate\"].dt.strftime(\"%Y-%m-%d\")\n",
    "    else:\n",
    "        # If it's already a string, normalize by slicing the date part (optional)\n",
    "        df_omr[\"chartdate\"] = df_omr[\"chartdate\"].astype(str).str[:10]\n",
    "\n",
    "# Clean integer columns\n",
    "for c in [\"subject_id\", \"seq_num\"]:\n",
    "    if c in df_omr.columns:\n",
    "        df_omr[c] = pd.to_numeric(df_omr[c], errors=\"coerce\").astype(\"Int64\")\n",
    "\n",
    "# Insert into SQLite\n",
    "with conn:\n",
    "    df_omr.to_sql(\"omr\", conn, if_exists=\"append\", index=False)\n",
    "\n",
    "# Quick verification\n",
    "rows = conn.execute(\"SELECT COUNT(*) FROM omr;\").fetchone()[0]\n",
    "print(f\"✅ Loaded {rows} rows into 'omr'.\")\n",
    "display(pd.read_sql(\"SELECT * FROM omr LIMIT 5;\", conn))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ba4dbd",
   "metadata": {},
   "source": [
    "### 2.1.02 `Hosp: Provider`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fcc37b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CREATE TABLE: provider ===\n",
    "schema_sql_provider = \"\"\"\n",
    "DROP TABLE IF EXISTS provider;\n",
    "CREATE TABLE provider (\n",
    "    provider_id TEXT NOT NULL\n",
    ");\n",
    "/* Optional index if provider_id is often used for joins */\n",
    "-- CREATE UNIQUE INDEX IF NOT EXISTS idx_provider_id ON provider(provider_id);\n",
    "\"\"\"\n",
    "\n",
    "with conn:\n",
    "    conn.executescript(schema_sql_provider)\n",
    "print(\"✅ Table 'provider' created.\")\n",
    "+\n",
    "# === LOAD CSV -> provider ===\n",
    "# Adjust CSV path as needed; expects settings.provider_csv to exist\n",
    "provider_csv_path = getattr(settings, \"provider_csv\", None)\n",
    "\n",
    "if provider_csv_path is None:\n",
    "    raise ValueError(\"⚠️ Please define settings.provider_csv with the path to your provider CSV file.\")\n",
    "\n",
    "# Read CSV\n",
    "df_provider = pd.read_csv(provider_csv_path, dtype=str)  # ensure provider_id stays as text\n",
    "\n",
    "# Normalize column name (handle variations like 'PROVIDER_ID')\n",
    "rename_map = {\"PROVIDER_ID\": \"provider_id\"}\n",
    "df_provider = df_provider.rename(columns={c: c.lower() for c in df_provider.columns})\n",
    "df_provider = df_provider.rename(columns=rename_map)\n",
    "\n",
    "# Validate expected column\n",
    "if \"provider_id\" not in df_provider.columns:\n",
    "    raise ValueError(\"⚠️ The provider CSV must contain a 'provider_id' column.\")\n",
    "\n",
    "# Keep only that column (avoid extra)\n",
    "df_provider = df_provider[[\"provider_id\"]]\n",
    "\n",
    "# Drop duplicates if any\n",
    "df_provider = df_provider.drop_duplicates()\n",
    "\n",
    "# Insert into SQLite\n",
    "with conn:\n",
    "    df_provider.to_sql(\"provider\", conn, if_exists=\"append\", index=False)\n",
    "\n",
    "# Quick verification\n",
    "count = conn.execute(\"SELECT COUNT(*) FROM provider;\").fetchone()[0]\n",
    "print(f\"✅ Loaded {count} rows into 'provider'.\")\n",
    "display(pd.read_sql(\"SELECT * FROM provider LIMIT 5;\", conn))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40c675dc",
   "metadata": {},
   "source": [
    "### 2.1.03 `Hosp: Admission`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d86102",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "\n",
    "# === CONFIG: set your paths here ===\n",
    "db_dir   = settings.DB_path.parent                         # folder where the DB will live\n",
    "csv_path = settings.admission_csv       # <-- change to your CSV path\n",
    "\n",
    "db_dir.mkdir(parents=True, exist_ok=True)\n",
    "db_path = settings.DB_path            # database filename\n",
    "\n",
    "# === 1) Create the database and the admissions table ===\n",
    "schema_sql = \"\"\"\n",
    "DROP TABLE IF EXISTS admissions;\n",
    "CREATE TABLE admissions (\n",
    "    subject_id             INTEGER NOT NULL,\n",
    "    hadm_id                INTEGER NOT NULL,\n",
    "    admittime              TEXT    NOT NULL,   -- store TIMESTAMP as ISO-8601 text\n",
    "    dischtime              TEXT,\n",
    "    deathtime              TEXT,\n",
    "    admission_type         TEXT    NOT NULL,\n",
    "    admit_provider_id      TEXT,\n",
    "    admission_location     TEXT,\n",
    "    discharge_location     TEXT,\n",
    "    insurance              TEXT,\n",
    "    language               TEXT,\n",
    "    marital_status         TEXT,\n",
    "    race                   TEXT,\n",
    "    edregtime              TEXT,\n",
    "    edouttime              TEXT,\n",
    "    hospital_expire_flag   INTEGER\n",
    ");\n",
    "-- Optional helpful indexes (uncomment if you want them)\n",
    "-- CREATE INDEX idx_admissions_hadm ON admissions(hadm_id);\n",
    "-- CREATE INDEX idx_admissions_subject ON admissions(subject_id);\n",
    "\"\"\"\n",
    "\n",
    "conn = sqlite3.connect(db_path)\n",
    "with conn:\n",
    "    conn.executescript(schema_sql)\n",
    "\n",
    "print(f\"✅ Created database at: {db_path.resolve()}\")\n",
    "print(\"✅ Created table: admissions\")\n",
    "\n",
    "# === 2) Load CSV and insert into admissions ===\n",
    "# If your CSV columns already match exactly, this will just work.\n",
    "# Otherwise, you can rename columns via the 'rename_map' below.\n",
    "parse_date_cols = [\n",
    "    \"admittime\", \"dischtime\", \"deathtime\", \"edregtime\", \"edouttime\"\n",
    "]\n",
    "\n",
    "# Read CSV; if your timestamps are ISO-8601 already, you can set dtype=str instead of parse_dates\n",
    "try:\n",
    "    df = pd.read_csv(csv_path, parse_dates=parse_date_cols, infer_datetime_format=True, keep_date_col=True)\n",
    "except ValueError:\n",
    "    # Fall back if some timestamp cols are missing in the CSV\n",
    "    df = pd.read_csv(csv_path)\n",
    "\n",
    "# Ensure column names match the target table\n",
    "expected_cols = [\n",
    "    \"subject_id\",\"hadm_id\",\"admittime\",\"dischtime\",\"deathtime\",\n",
    "    \"admission_type\",\"admit_provider_id\",\"admission_location\",\"discharge_location\",\n",
    "    \"insurance\",\"language\",\"marital_status\",\"race\",\"edregtime\",\"edouttime\",\n",
    "    \"hospital_expire_flag\"\n",
    "]\n",
    "\n",
    "# If your CSV headers differ, map them here, e.g. {'SUBJECT_ID':'subject_id', 'HADM_ID':'hadm_id', ...}\n",
    "rename_map = {}\n",
    "if rename_map:\n",
    "    df = df.rename(columns=rename_map)\n",
    "\n",
    "# Keep only expected columns (and in the right order)\n",
    "missing = [c for c in expected_cols if c not in df.columns]\n",
    "if missing:\n",
    "    raise ValueError(f\"The CSV is missing required columns: {missing}\")\n",
    "\n",
    "df = df[expected_cols]\n",
    "\n",
    "# Convert datetime columns to ISO 8601 strings for SQLite (TEXT)\n",
    "for c in [\"admittime\",\"dischtime\",\"deathtime\",\"edregtime\",\"edouttime\"]:\n",
    "    if c in df.columns:\n",
    "        # Convert to string only where not null\n",
    "        if pd.api.types.is_datetime64_any_dtype(df[c]):\n",
    "            df[c] = df[c].dt.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        else:\n",
    "            # If already strings, leave as-is\n",
    "            pass\n",
    "\n",
    "# Enforce integer types where appropriate (SQLite is flexible, but this helps cleanliness)\n",
    "int_cols = [\"subject_id\",\"hadm_id\",\"hospital_expire_flag\"]\n",
    "for c in int_cols:\n",
    "    if c in df.columns:\n",
    "        df[c] = pd.to_numeric(df[c], errors=\"coerce\").astype(\"Int64\")\n",
    "\n",
    "# Insert into the table\n",
    "# We defined the table explicitly, so we APPEND.\n",
    "with conn:\n",
    "    df.to_sql(\"admissions\", conn, if_exists=\"append\", index=False)\n",
    "\n",
    "# Quick verification\n",
    "row_count = conn.execute(\"SELECT COUNT(*) FROM admissions\").fetchone()[0]\n",
    "print(f\"✅ Inserted {row_count} rows into admissions\")\n",
    "\n",
    "# Peek at a few rows\n",
    "preview = pd.read_sql(\"SELECT * FROM admissions LIMIT 5;\", conn)\n",
    "conn.close()\n",
    "preview\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "163c6652",
   "metadata": {},
   "source": [
    "### 2.1.04 `Hosp: d_hcpcs`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf31c925",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CREATE TABLE: d_hcpcs ===\n",
    "schema_sql_d_hcpcs = \"\"\"\n",
    "DROP TABLE IF EXISTS d_hcpcs;\n",
    "CREATE TABLE d_hcpcs (\n",
    "    code               TEXT NOT NULL,   -- CHAR(5) in Postgres, stored as TEXT in SQLite\n",
    "    category           INTEGER,\n",
    "    long_description   TEXT,\n",
    "    short_description  TEXT\n",
    ");\n",
    "/* Optional index if codes are used for lookups */\n",
    "-- CREATE UNIQUE INDEX IF NOT EXISTS idx_d_hcpcs_code ON d_hcpcs(code);\n",
    "\"\"\"\n",
    "\n",
    "with conn:\n",
    "    conn.executescript(schema_sql_d_hcpcs)\n",
    "print(\"✅ Table 'd_hcpcs' created.\")\n",
    "\n",
    "# === LOAD CSV -> d_hcpcs ===\n",
    "d_hcpcs_csv_path = getattr(settings, \"d_hcpcs_csv\", None)\n",
    "if d_hcpcs_csv_path is None:\n",
    "    raise ValueError(\"⚠️ Please define settings.d_hcpcs_csv with the path to your d_hcpcs CSV file.\")\n",
    "\n",
    "# Read CSV (always treat code as string)\n",
    "df_d_hcpcs = pd.read_csv(d_hcpcs_csv_path, dtype=str)\n",
    "\n",
    "# Normalize column names (handle uppercase/lowercase mismatches)\n",
    "rename_map = {\n",
    "    \"CODE\": \"code\",\n",
    "    \"CATEGORY\": \"category\",\n",
    "    \"LONG_DESCRIPTION\": \"long_description\",\n",
    "    \"SHORT_DESCRIPTION\": \"short_description\"\n",
    "}\n",
    "df_d_hcpcs = df_d_hcpcs.rename(columns={c: c.lower() for c in df_d_hcpcs.columns})\n",
    "df_d_hcpcs = df_d_hcpcs.rename(columns=rename_map)\n",
    "\n",
    "# Verify required columns\n",
    "expected_cols = [\"code\", \"category\", \"long_description\", \"short_description\"]\n",
    "missing = [c for c in expected_cols if c not in df_d_hcpcs.columns]\n",
    "if missing:\n",
    "    raise ValueError(f\"⚠️ d_hcpcs CSV is missing required columns: {missing}\")\n",
    "\n",
    "# Keep only expected columns and enforce types\n",
    "df_d_hcpcs = df_d_hcpcs[expected_cols]\n",
    "\n",
    "# Convert category to integer where possible\n",
    "df_d_hcpcs[\"category\"] = pd.to_numeric(df_d_hcpcs[\"category\"], errors=\"coerce\").astype(\"Int64\")\n",
    "\n",
    "# Drop duplicate codes if any\n",
    "df_d_hcpcs = df_d_hcpcs.drop_duplicates(subset=[\"code\"])\n",
    "\n",
    "# Insert into SQLite\n",
    "with conn:\n",
    "    df_d_hcpcs.to_sql(\"d_hcpcs\", conn, if_exists=\"append\", index=False)\n",
    "\n",
    "# Quick verification\n",
    "count = conn.execute(\"SELECT COUNT(*) FROM d_hcpcs;\").fetchone()[0]\n",
    "print(f\"✅ Loaded {count} rows into 'd_hcpcs'.\")\n",
    "display(pd.read_sql(\"SELECT * FROM d_hcpcs LIMIT 5;\", conn))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e62838b",
   "metadata": {},
   "source": [
    "### 2.1.05 `Hosp: d_icd_diagnoses`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb0cf544",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CREATE TABLE: d_icd_diagnoses ===\n",
    "schema_sql_d_icd_diagnoses = \"\"\"\n",
    "DROP TABLE IF EXISTS d_icd_diagnoses;\n",
    "CREATE TABLE d_icd_diagnoses (\n",
    "    icd_code     TEXT    NOT NULL,   -- CHAR(7) in Postgres → TEXT in SQLite\n",
    "    icd_version  INTEGER NOT NULL,\n",
    "    long_title   TEXT\n",
    ");\n",
    "/* Optional index for faster ICD code lookups */\n",
    "-- CREATE UNIQUE INDEX IF NOT EXISTS idx_d_icd_diagnoses_code ON d_icd_diagnoses(icd_code, icd_version);\n",
    "\"\"\"\n",
    "\n",
    "with conn:\n",
    "    conn.executescript(schema_sql_d_icd_diagnoses)\n",
    "print(\"✅ Table 'd_icd_diagnoses' created.\")\n",
    "\n",
    "# === LOAD CSV -> d_icd_diagnoses ===\n",
    "d_icd_diagnoses_csv_path = getattr(settings, \"d_icd_diagnoses_csv\", None)\n",
    "if d_icd_diagnoses_csv_path is None:\n",
    "    raise ValueError(\"⚠️ Please define settings.d_icd_diagnoses_csv with the path to your d_icd_diagnoses CSV file.\")\n",
    "\n",
    "# Read CSV (keep all as strings first)\n",
    "df_d_icd_diagnoses = pd.read_csv(d_icd_diagnoses_csv_path, dtype=str)\n",
    "\n",
    "# Normalize column names (handle uppercase/lowercase mismatches)\n",
    "rename_map = {\n",
    "    \"ICD_CODE\": \"icd_code\",\n",
    "    \"ICD_VERSION\": \"icd_version\",\n",
    "    \"LONG_TITLE\": \"long_title\"\n",
    "}\n",
    "df_d_icd_diagnoses = df_d_icd_diagnoses.rename(columns={c: c.lower() for c in df_d_icd_diagnoses.columns})\n",
    "df_d_icd_diagnoses = df_d_icd_diagnoses.rename(columns=rename_map)\n",
    "\n",
    "# Validate required columns\n",
    "expected_cols = [\"icd_code\", \"icd_version\", \"long_title\"]\n",
    "missing = [c for c in expected_cols if c not in df_d_icd_diagnoses.columns]\n",
    "if missing:\n",
    "    raise ValueError(f\"⚠️ d_icd_diagnoses CSV is missing required columns: {missing}\")\n",
    "\n",
    "# Keep only expected columns in order\n",
    "df_d_icd_diagnoses = df_d_icd_diagnoses[expected_cols]\n",
    "\n",
    "# Convert icd_version to integer\n",
    "df_d_icd_diagnoses[\"icd_version\"] = pd.to_numeric(df_d_icd_diagnoses[\"icd_version\"], errors=\"coerce\").astype(\"Int64\")\n",
    "\n",
    "# Drop duplicates (by code & version)\n",
    "df_d_icd_diagnoses = df_d_icd_diagnoses.drop_duplicates(subset=[\"icd_code\", \"icd_version\"])\n",
    "\n",
    "# Insert into SQLite\n",
    "with conn:\n",
    "    df_d_icd_diagnoses.to_sql(\"d_icd_diagnoses\", conn, if_exists=\"append\", index=False)\n",
    "\n",
    "# Quick verification\n",
    "count = conn.execute(\"SELECT COUNT(*) FROM d_icd_diagnoses;\").fetchone()[0]\n",
    "print(f\"✅ Loaded {count} rows into 'd_icd_diagnoses'.\")\n",
    "display(pd.read_sql(\"SELECT * FROM d_icd_diagnoses LIMIT 5;\", conn))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a546a34",
   "metadata": {},
   "source": [
    "### 2.1.06 `Hosp: d_icd_procedures`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda7bdbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CREATE TABLE: d_icd_procedures ===\n",
    "schema_sql_d_icd_procedures = \"\"\"\n",
    "DROP TABLE IF EXISTS d_icd_procedures;\n",
    "CREATE TABLE d_icd_procedures (\n",
    "    icd_code     TEXT    NOT NULL,   -- CHAR(7) in Postgres → TEXT in SQLite\n",
    "    icd_version  INTEGER NOT NULL,\n",
    "    long_title   TEXT\n",
    ");\n",
    "/* Optional index for faster lookups */\n",
    "-- CREATE UNIQUE INDEX IF NOT EXISTS idx_d_icd_procedures_code ON d_icd_procedures(icd_code, icd_version);\n",
    "\"\"\"\n",
    "\n",
    "with conn:\n",
    "    conn.executescript(schema_sql_d_icd_procedures)\n",
    "print(\"✅ Table 'd_icd_procedures' created.\")\n",
    "\n",
    "# === LOAD CSV -> d_icd_procedures ===\n",
    "d_icd_procedures_csv_path = getattr(settings, \"d_icd_procedures_csv\", None)\n",
    "if d_icd_procedures_csv_path is None:\n",
    "    raise ValueError(\"⚠️ Please define settings.d_icd_procedures_csv with the path to your d_icd_procedures CSV file.\")\n",
    "\n",
    "# Read CSV (keep everything as text initially)\n",
    "df_d_icd_procedures = pd.read_csv(d_icd_procedures_csv_path, dtype=str)\n",
    "\n",
    "# Normalize and rename columns to match schema\n",
    "rename_map = {\n",
    "    \"ICD_CODE\": \"icd_code\",\n",
    "    \"ICD_VERSION\": \"icd_version\",\n",
    "    \"LONG_TITLE\": \"long_title\"\n",
    "}\n",
    "df_d_icd_procedures = df_d_icd_procedures.rename(columns={c: c.lower() for c in df_d_icd_procedures.columns})\n",
    "df_d_icd_procedures = df_d_icd_procedures.rename(columns=rename_map)\n",
    "\n",
    "# Verify columns\n",
    "expected_cols = [\"icd_code\", \"icd_version\", \"long_title\"]\n",
    "missing = [c for c in expected_cols if c not in df_d_icd_procedures.columns]\n",
    "if missing:\n",
    "    raise ValueError(f\"⚠️ d_icd_procedures CSV is missing required columns: {missing}\")\n",
    "\n",
    "# Keep only expected columns\n",
    "df_d_icd_procedures = df_d_icd_procedures[expected_cols]\n",
    "\n",
    "# Convert icd_version to integer (nullable-safe)\n",
    "df_d_icd_procedures[\"icd_version\"] = pd.to_numeric(df_d_icd_procedures[\"icd_version\"], errors=\"coerce\").astype(\"Int64\")\n",
    "\n",
    "# Drop duplicates by (icd_code, icd_version)\n",
    "df_d_icd_procedures = df_d_icd_procedures.drop_duplicates(subset=[\"icd_code\", \"icd_version\"])\n",
    "\n",
    "# Insert into SQLite\n",
    "with conn:\n",
    "    df_d_icd_procedures.to_sql(\"d_icd_procedures\", conn, if_exists=\"append\", index=False)\n",
    "\n",
    "# Quick verification\n",
    "count = conn.execute(\"SELECT COUNT(*) FROM d_icd_procedures;\").fetchone()[0]\n",
    "print(f\"✅ Loaded {count} rows into 'd_icd_procedures'.\")\n",
    "display(pd.read_sql(\"SELECT * FROM d_icd_procedures LIMIT 5;\", conn))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63bf376c",
   "metadata": {},
   "source": [
    "### 2.1.07 `Hosp: d_labitems`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f275a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CREATE TABLE: d_labitems ===\n",
    "schema_sql_d_labitems = \"\"\"\n",
    "DROP TABLE IF EXISTS d_labitems;\n",
    "CREATE TABLE d_labitems (\n",
    "    itemid    INTEGER,\n",
    "    label     TEXT,\n",
    "    fluid     TEXT,\n",
    "    category  TEXT\n",
    ");\n",
    "/* Optional index for fast lookup by itemid */\n",
    "-- CREATE UNIQUE INDEX IF NOT EXISTS idx_d_labitems_itemid ON d_labitems(itemid);\n",
    "\"\"\"\n",
    "\n",
    "with conn:\n",
    "    conn.executescript(schema_sql_d_labitems)\n",
    "print(\"✅ Table 'd_labitems' created.\")\n",
    "\n",
    "# === LOAD CSV -> d_labitems ===\n",
    "d_labitems_csv_path = getattr(settings, \"d_labitems_csv\", None)\n",
    "if d_labitems_csv_path is None:\n",
    "    raise ValueError(\"⚠️ Please define settings.d_labitems_csv with the path to your d_labitems CSV file.\")\n",
    "\n",
    "# Read CSV\n",
    "df_d_labitems = pd.read_csv(d_labitems_csv_path, dtype=str)  # start as string to avoid parsing issues\n",
    "\n",
    "# Normalize column names (handle uppercase/lowercase differences)\n",
    "rename_map = {\n",
    "    \"ITEMID\": \"itemid\",\n",
    "    \"LABEL\": \"label\",\n",
    "    \"FLUID\": \"fluid\",\n",
    "    \"CATEGORY\": \"category\"\n",
    "}\n",
    "df_d_labitems = df_d_labitems.rename(columns={c: c.lower() for c in df_d_labitems.columns})\n",
    "df_d_labitems = df_d_labitems.rename(columns=rename_map)\n",
    "\n",
    "# Verify required columns\n",
    "expected_cols = [\"itemid\", \"label\", \"fluid\", \"category\"]\n",
    "missing = [c for c in expected_cols if c not in df_d_labitems.columns]\n",
    "if missing:\n",
    "    raise ValueError(f\"⚠️ d_labitems CSV is missing required columns: {missing}\")\n",
    "\n",
    "# Keep only expected columns\n",
    "df_d_labitems = df_d_labitems[expected_cols]\n",
    "\n",
    "# Convert itemid to integer (nullable-safe)\n",
    "df_d_labitems[\"itemid\"] = pd.to_numeric(df_d_labitems[\"itemid\"], errors=\"coerce\").astype(\"Int64\")\n",
    "\n",
    "# Drop duplicates (by itemid if present)\n",
    "df_d_labitems = df_d_labitems.drop_duplicates(subset=[\"itemid\"])\n",
    "\n",
    "# Insert into SQLite\n",
    "with conn:\n",
    "    df_d_labitems.to_sql(\"d_labitems\", conn, if_exists=\"append\", index=False)\n",
    "\n",
    "# Quick verification\n",
    "count = conn.execute(\"SELECT COUNT(*) FROM d_labitems;\").fetchone()[0]\n",
    "print(f\"✅ Loaded {count} rows into 'd_labitems'.\")\n",
    "display(pd.read_sql(\"SELECT * FROM d_labitems LIMIT 5;\", conn))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f0fc4b1",
   "metadata": {},
   "source": [
    "### 2.1.08 `Hosp: diagnoses_icd`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb3c224",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CREATE TABLE: diagnoses_icd ===\n",
    "schema_sql_diagnoses_icd = \"\"\"\n",
    "DROP TABLE IF EXISTS diagnoses_icd;\n",
    "CREATE TABLE diagnoses_icd (\n",
    "    subject_id   INTEGER NOT NULL,\n",
    "    hadm_id      INTEGER NOT NULL,\n",
    "    seq_num      INTEGER NOT NULL,\n",
    "    icd_code     TEXT,\n",
    "    icd_version  INTEGER\n",
    ");\n",
    "/* Optional indexes to speed up lookups */\n",
    "-- CREATE INDEX IF NOT EXISTS idx_diagnoses_subject ON diagnoses_icd(subject_id);\n",
    "-- CREATE INDEX IF NOT EXISTS idx_diagnoses_hadm ON diagnoses_icd(hadm_id);\n",
    "-- CREATE INDEX IF NOT EXISTS idx_diagnoses_code ON diagnoses_icd(icd_code);\n",
    "\"\"\"\n",
    "\n",
    "with conn:\n",
    "    conn.executescript(schema_sql_diagnoses_icd)\n",
    "print(\"✅ Table 'diagnoses_icd' created.\")\n",
    "\n",
    "# === LOAD CSV -> diagnoses_icd ===\n",
    "diagnoses_icd_csv_path = getattr(settings, \"diagnoses_icd_csv\", None)\n",
    "if diagnoses_icd_csv_path is None:\n",
    "    raise ValueError(\"⚠️ Please define settings.diagnoses_icd_csv with the path to your diagnoses_icd CSV file.\")\n",
    "\n",
    "# Read CSV (treat all as strings first)\n",
    "df_diagnoses_icd = pd.read_csv(diagnoses_icd_csv_path, dtype=str)\n",
    "\n",
    "# Normalize column names (handle upper/lowercase differences)\n",
    "rename_map = {\n",
    "    \"SUBJECT_ID\": \"subject_id\",\n",
    "    \"HADM_ID\": \"hadm_id\",\n",
    "    \"SEQ_NUM\": \"seq_num\",\n",
    "    \"ICD_CODE\": \"icd_code\",\n",
    "    \"ICD_VERSION\": \"icd_version\"\n",
    "}\n",
    "df_diagnoses_icd = df_diagnoses_icd.rename(columns={c: c.lower() for c in df_diagnoses_icd.columns})\n",
    "df_diagnoses_icd = df_diagnoses_icd.rename(columns=rename_map)\n",
    "\n",
    "# Validate required columns\n",
    "expected_cols = [\"subject_id\", \"hadm_id\", \"seq_num\", \"icd_code\", \"icd_version\"]\n",
    "missing = [c for c in expected_cols if c not in df_diagnoses_icd.columns]\n",
    "if missing:\n",
    "    raise ValueError(f\"⚠️ diagnoses_icd CSV is missing required columns: {missing}\")\n",
    "\n",
    "# Keep only expected columns\n",
    "df_diagnoses_icd = df_diagnoses_icd[expected_cols]\n",
    "\n",
    "# Convert numeric columns safely\n",
    "for col in [\"subject_id\", \"hadm_id\", \"seq_num\", \"icd_version\"]:\n",
    "    df_diagnoses_icd[col] = pd.to_numeric(df_diagnoses_icd[col], errors=\"coerce\").astype(\"Int64\")\n",
    "\n",
    "# Drop duplicates (if any)\n",
    "df_diagnoses_icd = df_diagnoses_icd.drop_duplicates(subset=[\"subject_id\", \"hadm_id\", \"seq_num\"])\n",
    "\n",
    "# Insert into SQLite\n",
    "with conn:\n",
    "    df_diagnoses_icd.to_sql(\"diagnoses_icd\", conn, if_exists=\"append\", index=False)\n",
    "\n",
    "# Quick verification\n",
    "count = conn.execute(\"SELECT COUNT(*) FROM diagnoses_icd;\").fetchone()[0]\n",
    "print(f\"✅ Loaded {count} rows into 'diagnoses_icd'.\")\n",
    "display(pd.read_sql(\"SELECT * FROM diagnoses_icd LIMIT 5;\", conn))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e099439",
   "metadata": {},
   "source": [
    "### 2.1.09 `Hosp: drgcodes`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "795fc476",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CREATE TABLE: drgcodes ===\n",
    "schema_sql_drgcodes = \"\"\"\n",
    "DROP TABLE IF EXISTS drgcodes;\n",
    "CREATE TABLE drgcodes (\n",
    "    subject_id     INTEGER,\n",
    "    hadm_id        INTEGER,\n",
    "    drg_type       TEXT,\n",
    "    drg_code       TEXT,\n",
    "    description    TEXT,\n",
    "    drg_severity   INTEGER,\n",
    "    drg_mortality  INTEGER\n",
    ");\n",
    "/* Optional indexes for faster joins or lookups */\n",
    "-- CREATE INDEX IF NOT EXISTS idx_drgcodes_subject ON drgcodes(subject_id);\n",
    "-- CREATE INDEX IF NOT EXISTS idx_drgcodes_hadm ON drgcodes(hadm_id);\n",
    "-- CREATE INDEX IF NOT EXISTS idx_drgcodes_code ON drgcodes(drg_code);\n",
    "\"\"\"\n",
    "\n",
    "with conn:\n",
    "    conn.executescript(schema_sql_drgcodes)\n",
    "print(\"✅ Table 'drgcodes' created.\")\n",
    "\n",
    "# === LOAD CSV -> drgcodes ===\n",
    "drgcodes_csv_path = getattr(settings, \"drgcodes_csv\", None)\n",
    "if drgcodes_csv_path is None:\n",
    "    raise ValueError(\"⚠️ Please define settings.drgcodes_csv with the path to your drgcodes CSV file.\")\n",
    "\n",
    "# Read CSV as strings (safer for mixed types)\n",
    "df_drgcodes = pd.read_csv(drgcodes_csv_path, dtype=str)\n",
    "\n",
    "# Normalize and rename columns\n",
    "rename_map = {\n",
    "    \"SUBJECT_ID\": \"subject_id\",\n",
    "    \"HADM_ID\": \"hadm_id\",\n",
    "    \"DRG_TYPE\": \"drg_type\",\n",
    "    \"DRG_CODE\": \"drg_code\",\n",
    "    \"DESCRIPTION\": \"description\",\n",
    "    \"DRG_SEVERITY\": \"drg_severity\",\n",
    "    \"DRG_MORTALITY\": \"drg_mortality\"\n",
    "}\n",
    "df_drgcodes = df_drgcodes.rename(columns={c: c.lower() for c in df_drgcodes.columns})\n",
    "df_drgcodes = df_drgcodes.rename(columns=rename_map)\n",
    "\n",
    "# Validate required columns\n",
    "expected_cols = [\"subject_id\", \"hadm_id\", \"drg_type\", \"drg_code\", \"description\", \"drg_severity\", \"drg_mortality\"]\n",
    "missing = [c for c in expected_cols if c not in df_drgcodes.columns]\n",
    "if missing:\n",
    "    raise ValueError(f\"⚠️ drgcodes CSV is missing required columns: {missing}\")\n",
    "\n",
    "# Keep only expected columns\n",
    "df_drgcodes = df_drgcodes[expected_cols]\n",
    "\n",
    "# Convert integer columns safely\n",
    "for col in [\"subject_id\", \"hadm_id\", \"drg_severity\", \"drg_mortality\"]:\n",
    "    df_drgcodes[col] = pd.to_numeric(df_drgcodes[col], errors=\"coerce\").astype(\"Int64\")\n",
    "\n",
    "# Drop duplicates\n",
    "df_drgcodes = df_drgcodes.drop_duplicates(subset=[\"subject_id\", \"hadm_id\", \"drg_code\"])\n",
    "\n",
    "# Insert into SQLite\n",
    "with conn:\n",
    "    df_drgcodes.to_sql(\"drgcodes\", conn, if_exists=\"append\", index=False)\n",
    "\n",
    "# Quick verification\n",
    "count = conn.execute(\"SELECT COUNT(*) FROM drgcodes;\").fetchone()[0]\n",
    "print(f\"✅ Loaded {count} rows into 'drgcodes'.\")\n",
    "display(pd.read_sql(\"SELECT * FROM drgcodes LIMIT 5;\", conn))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31cf2320",
   "metadata": {},
   "source": [
    "### 2.1.10 `Hosp: emar`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d97e09b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CREATE TABLE: emar ===\n",
    "schema_sql_emar = \"\"\"\n",
    "DROP TABLE IF EXISTS emar;\n",
    "CREATE TABLE emar (\n",
    "    subject_id          INTEGER NOT NULL,\n",
    "    hadm_id             INTEGER,\n",
    "    emar_id             TEXT    NOT NULL,\n",
    "    emar_seq            INTEGER NOT NULL,\n",
    "    poe_id              TEXT    NOT NULL,\n",
    "    pharmacy_id         INTEGER,\n",
    "    enter_provider_id   TEXT,\n",
    "    charttime           TEXT    NOT NULL,  -- store TIMESTAMP as ISO8601 text\n",
    "    medication          TEXT,\n",
    "    event_txt           TEXT,\n",
    "    scheduletime        TEXT,\n",
    "    storetime           TEXT    NOT NULL\n",
    ");\n",
    "/* Optional indexes for faster lookups */\n",
    "-- CREATE INDEX IF NOT EXISTS idx_emar_subject ON emar(subject_id);\n",
    "-- CREATE INDEX IF NOT EXISTS idx_emar_hadm ON emar(hadm_id);\n",
    "-- CREATE INDEX IF NOT EXISTS idx_emar_charttime ON emar(charttime);\n",
    "\"\"\"\n",
    "\n",
    "with conn:\n",
    "    conn.executescript(schema_sql_emar)\n",
    "print(\"✅ Table 'emar' created.\")\n",
    "\n",
    "# === LOAD CSV -> emar ===\n",
    "emar_csv_path = getattr(settings, \"emar_csv\", None)\n",
    "if emar_csv_path is None:\n",
    "    raise ValueError(\"⚠️ Please define settings.emar_csv with the path to your emar CSV file.\")\n",
    "\n",
    "# Read CSV as strings (timestamps converted later)\n",
    "df_emar = pd.read_csv(emar_csv_path, dtype=str)\n",
    "\n",
    "# Normalize and rename columns to lowercase\n",
    "rename_map = {\n",
    "    \"SUBJECT_ID\": \"subject_id\",\n",
    "    \"HADM_ID\": \"hadm_id\",\n",
    "    \"EMAR_ID\": \"emar_id\",\n",
    "    \"EMAR_SEQ\": \"emar_seq\",\n",
    "    \"POE_ID\": \"poe_id\",\n",
    "    \"PHARMACY_ID\": \"pharmacy_id\",\n",
    "    \"ENTER_PROVIDER_ID\": \"enter_provider_id\",\n",
    "    \"CHARTTIME\": \"charttime\",\n",
    "    \"MEDICATION\": \"medication\",\n",
    "    \"EVENT_TXT\": \"event_txt\",\n",
    "    \"SCHEDULETIME\": \"scheduletime\",\n",
    "    \"STORETIME\": \"storetime\"\n",
    "}\n",
    "df_emar = df_emar.rename(columns={c: c.lower() for c in df_emar.columns})\n",
    "df_emar = df_emar.rename(columns=rename_map)\n",
    "\n",
    "# Verify required columns\n",
    "expected_cols = [\n",
    "    \"subject_id\", \"hadm_id\", \"emar_id\", \"emar_seq\", \"poe_id\", \"pharmacy_id\",\n",
    "    \"enter_provider_id\", \"charttime\", \"medication\", \"event_txt\",\n",
    "    \"scheduletime\", \"storetime\"\n",
    "]\n",
    "missing = [c for c in expected_cols if c not in df_emar.columns]\n",
    "if missing:\n",
    "    raise ValueError(f\"⚠️ emar CSV is missing required columns: {missing}\")\n",
    "\n",
    "# Keep only expected columns\n",
    "df_emar = df_emar[expected_cols]\n",
    "\n",
    "# Convert numeric columns safely\n",
    "for col in [\"subject_id\", \"hadm_id\", \"emar_seq\", \"pharmacy_id\"]:\n",
    "    df_emar[col] = pd.to_numeric(df_emar[col], errors=\"coerce\").astype(\"Int64\")\n",
    "\n",
    "# Convert timestamps to ISO 8601 text\n",
    "for col in [\"charttime\", \"scheduletime\", \"storetime\"]:\n",
    "    if col in df_emar.columns:\n",
    "        df_emar[col] = pd.to_datetime(df_emar[col], errors=\"coerce\").dt.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "# Drop duplicates if any\n",
    "df_emar = df_emar.drop_duplicates(subset=[\"emar_id\", \"emar_seq\"])\n",
    "\n",
    "# Insert into SQLite\n",
    "with conn:\n",
    "    df_emar.to_sql(\"emar\", conn, if_exists=\"append\", index=False)\n",
    "\n",
    "# Quick verification\n",
    "count = conn.execute(\"SELECT COUNT(*) FROM emar;\").fetchone()[0]\n",
    "print(f\"✅ Loaded {count} rows into 'emar'.\")\n",
    "display(pd.read_sql(\"SELECT * FROM emar LIMIT 5;\", conn))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c90a2b07",
   "metadata": {},
   "source": [
    "### 2.1.11 `Hosp: emar_detail`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ce6c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CREATE TABLE: emar_detail ===\n",
    "schema_sql_emar_detail = \"\"\"\n",
    "DROP TABLE IF EXISTS emar_detail;\n",
    "CREATE TABLE emar_detail (\n",
    "    subject_id                               INTEGER NOT NULL,\n",
    "    emar_id                                  TEXT    NOT NULL,   -- VARCHAR(25)\n",
    "    emar_seq                                 INTEGER NOT NULL,\n",
    "    parent_field_ordinal                     TEXT,               -- VARCHAR(10)\n",
    "    administration_type                      TEXT,               -- VARCHAR(50)\n",
    "    pharmacy_id                              INTEGER,\n",
    "    barcode_type                             TEXT,               -- VARCHAR(4)\n",
    "    reason_for_no_barcode                    TEXT,\n",
    "    complete_dose_not_given                  TEXT,               -- VARCHAR(5)\n",
    "    dose_due                                 TEXT,               -- VARCHAR(100)\n",
    "    dose_due_unit                            TEXT,               -- VARCHAR(50)\n",
    "    dose_given                               TEXT,               -- VARCHAR(255)\n",
    "    dose_given_unit                          TEXT,               -- VARCHAR(50)\n",
    "    will_remainder_of_dose_be_given          TEXT,               -- VARCHAR(5)\n",
    "    product_amount_given                     TEXT,               -- VARCHAR(30)\n",
    "    product_unit                             TEXT,               -- VARCHAR(30)\n",
    "    product_code                             TEXT,               -- VARCHAR(30)\n",
    "    product_description                      TEXT,               -- VARCHAR(255)\n",
    "    product_description_other                TEXT,               -- VARCHAR(255)\n",
    "    prior_infusion_rate                      TEXT,               -- VARCHAR(40)\n",
    "    infusion_rate                            TEXT,               -- VARCHAR(40)\n",
    "    infusion_rate_adjustment                 TEXT,               -- VARCHAR(50)\n",
    "    infusion_rate_adjustment_amount          TEXT,               -- VARCHAR(30)\n",
    "    infusion_rate_unit                       TEXT,               -- VARCHAR(30)\n",
    "    route                                    TEXT,               -- VARCHAR(10)\n",
    "    infusion_complete                        TEXT,               -- VARCHAR(1)\n",
    "    completion_interval                      TEXT,               -- VARCHAR(50)\n",
    "    new_iv_bag_hung                          TEXT,               -- VARCHAR(1)\n",
    "    continued_infusion_in_other_location     TEXT,               -- VARCHAR(1)\n",
    "    restart_interval                         TEXT,\n",
    "    side                                     TEXT,               -- VARCHAR(10)\n",
    "    site                                     TEXT,               -- VARCHAR(255)\n",
    "    non_formulary_visual_verification        TEXT                -- VARCHAR(1)\n",
    ");\n",
    "/* Optional indexes */\n",
    "-- CREATE INDEX IF NOT EXISTS idx_emar_detail_subject ON emar_detail(subject_id);\n",
    "-- CREATE INDEX IF NOT EXISTS idx_emar_detail_emar ON emar_detail(emar_id, emar_seq);\n",
    "\"\"\"\n",
    "\n",
    "with conn:\n",
    "    conn.executescript(schema_sql_emar_detail)\n",
    "print(\"✅ Table 'emar_detail' created.\")\n",
    "\n",
    "# === LOAD CSV -> emar_detail ===\n",
    "emar_detail_csv_path = getattr(settings, \"emar_detail_csv\", None)\n",
    "if emar_detail_csv_path is None:\n",
    "    raise ValueError(\"⚠️ Please define settings.emar_detail_csv with the path to your emar_detail CSV file.\")\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(emar_detail_csv_path, dtype=str)\n",
    "\n",
    "# Normalize/rename columns to expected names\n",
    "rename_map = {\n",
    "    \"SUBJECT_ID\": \"subject_id\",\n",
    "    \"EMAR_ID\": \"emar_id\",\n",
    "    \"EMAR_SEQ\": \"emar_seq\",\n",
    "    \"PARENT_FIELD_ORDINAL\": \"parent_field_ordinal\",\n",
    "    \"ADMINISTRATION_TYPE\": \"administration_type\",\n",
    "    \"PHARMACY_ID\": \"pharmacy_id\",\n",
    "    \"BARCODE_TYPE\": \"barcode_type\",\n",
    "    \"REASON_FOR_NO_BARCODE\": \"reason_for_no_barcode\",\n",
    "    \"COMPLETE_DOSE_NOT_GIVEN\": \"complete_dose_not_given\",\n",
    "    \"DOSE_DUE\": \"dose_due\",\n",
    "    \"DOSE_DUE_UNIT\": \"dose_due_unit\",\n",
    "    \"DOSE_GIVEN\": \"dose_given\",\n",
    "    \"DOSE_GIVEN_UNIT\": \"dose_given_unit\",\n",
    "    \"WILL_REMAINDER_OF_DOSE_BE_GIVEN\": \"will_remainder_of_dose_be_given\",\n",
    "    \"PRODUCT_AMOUNT_GIVEN\": \"product_amount_given\",\n",
    "    \"PRODUCT_UNIT\": \"product_unit\",\n",
    "    \"PRODUCT_CODE\": \"product_code\",\n",
    "    \"PRODUCT_DESCRIPTION\": \"product_description\",\n",
    "    \"PRODUCT_DESCRIPTION_OTHER\": \"product_description_other\",\n",
    "    \"PRIOR_INFUSION_RATE\": \"prior_infusion_rate\",\n",
    "    \"INFUSION_RATE\": \"infusion_rate\",\n",
    "    \"INFUSION_RATE_ADJUSTMENT\": \"infusion_rate_adjustment\",\n",
    "    \"INFUSION_RATE_ADJUSTMENT_AMOUNT\": \"infusion_rate_adjustment_amount\",\n",
    "    \"INFUSION_RATE_UNIT\": \"infusion_rate_unit\",\n",
    "    \"ROUTE\": \"route\",\n",
    "    \"INFUSION_COMPLETE\": \"infusion_complete\",\n",
    "    \"COMPLETION_INTERVAL\": \"completion_interval\",\n",
    "    \"NEW_IV_BAG_HUNG\": \"new_iv_bag_hung\",\n",
    "    \"CONTINUED_INFUSION_IN_OTHER_LOCATION\": \"continued_infusion_in_other_location\",\n",
    "    \"RESTART_INTERVAL\": \"restart_interval\",\n",
    "    \"SIDE\": \"side\",\n",
    "    \"SITE\": \"site\",\n",
    "    \"NON_FORMULARY_VISUAL_VERIFICATION\": \"non_formulary_visual_verification\",\n",
    "}\n",
    "df = df.rename(columns={c: c.lower() for c in df.columns}).rename(columns=rename_map)\n",
    "\n",
    "expected_cols = [\n",
    "    \"subject_id\",\"emar_id\",\"emar_seq\",\"parent_field_ordinal\",\"administration_type\",\n",
    "    \"pharmacy_id\",\"barcode_type\",\"reason_for_no_barcode\",\"complete_dose_not_given\",\n",
    "    \"dose_due\",\"dose_due_unit\",\"dose_given\",\"dose_given_unit\",\"will_remainder_of_dose_be_given\",\n",
    "    \"product_amount_given\",\"product_unit\",\"product_code\",\"product_description\",\n",
    "    \"product_description_other\",\"prior_infusion_rate\",\"infusion_rate\",\"infusion_rate_adjustment\",\n",
    "    \"infusion_rate_adjustment_amount\",\"infusion_rate_unit\",\"route\",\"infusion_complete\",\n",
    "    \"completion_interval\",\"new_iv_bag_hung\",\"continued_infusion_in_other_location\",\n",
    "    \"restart_interval\",\"side\",\"site\",\"non_formulary_visual_verification\"\n",
    "]\n",
    "missing = [c for c in expected_cols if c not in df.columns]\n",
    "if missing:\n",
    "    raise ValueError(f\"⚠️ emar_detail CSV is missing required columns: {missing}\")\n",
    "\n",
    "df = df[expected_cols]\n",
    "\n",
    "# Cast ints safely\n",
    "for col in [\"subject_id\", \"emar_seq\", \"pharmacy_id\"]:\n",
    "    df[col] = pd.to_numeric(df[col], errors=\"coerce\").astype(\"Int64\")\n",
    "\n",
    "# Deduplicate (natural key)\n",
    "df = df.drop_duplicates(subset=[\"emar_id\", \"emar_seq\", \"parent_field_ordinal\"])\n",
    "\n",
    "with conn:\n",
    "    df.to_sql(\"emar_detail\", conn, if_exists=\"append\", index=False)\n",
    "\n",
    "count = conn.execute(\"SELECT COUNT(*) FROM emar_detail;\").fetchone()[0]\n",
    "print(f\"✅ Loaded {count} rows into 'emar_detail'.\")\n",
    "display(pd.read_sql(\"SELECT * FROM emar_detail LIMIT 5;\", conn))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff46e950",
   "metadata": {},
   "source": [
    "### 2.1.12 `Hosp: hcpcsevents`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8743136c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CREATE TABLE: hcpcsevents ===\n",
    "schema_sql_hcpcsevents = \"\"\"\n",
    "DROP TABLE IF EXISTS hcpcsevents;\n",
    "CREATE TABLE hcpcsevents (\n",
    "    subject_id        INTEGER NOT NULL,\n",
    "    hadm_id           INTEGER NOT NULL,\n",
    "    chartdate         TEXT,          -- DATE stored as 'YYYY-MM-DD'\n",
    "    hcpcs_cd          TEXT    NOT NULL,   -- CHAR(5)\n",
    "    seq_num           INTEGER NOT NULL,\n",
    "    short_description TEXT\n",
    ");\n",
    "/* Optional indexes */\n",
    "-- CREATE INDEX IF NOT EXISTS idx_hcpcs_subject ON hcpcsevents(subject_id);\n",
    "-- CREATE INDEX IF NOT EXISTS idx_hcpcs_hadm    ON hcpcsevents(hadm_id);\n",
    "-- CREATE INDEX IF NOT EXISTS idx_hcpcs_code    ON hcpcsevents(hcpcs_cd);\n",
    "\"\"\"\n",
    "\n",
    "with conn:\n",
    "    conn.executescript(schema_sql_hcpcsevents)\n",
    "print(\"✅ Table 'hcpcsevents' created.\")\n",
    "\n",
    "# === LOAD CSV -> hcpcsevents ===\n",
    "hcpcsevents_csv_path = getattr(settings, \"hcpcsevents_csv\", None)\n",
    "if hcpcsevents_csv_path is None:\n",
    "    raise ValueError(\"⚠️ Please define settings.hcpcsevents_csv with the path to your HCPCS events CSV file.\")\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Read as strings first (we'll parse date next)\n",
    "df_hcpcs = pd.read_csv(hcpcsevents_csv_path, dtype=str)\n",
    "\n",
    "# Normalize/rename columns to match schema\n",
    "rename_map = {\n",
    "    \"SUBJECT_ID\": \"subject_id\",\n",
    "    \"HADM_ID\": \"hadm_id\",\n",
    "    \"CHARTDATE\": \"chartdate\",\n",
    "    \"HCPCS_CD\": \"hcpcs_cd\",\n",
    "    \"SEQ_NUM\": \"seq_num\",\n",
    "    \"SHORT_DESCRIPTION\": \"short_description\",\n",
    "}\n",
    "df_hcpcs = df_hcpcs.rename(columns={c: c.lower() for c in df_hcpcs.columns}).rename(columns=rename_map)\n",
    "\n",
    "expected_cols = [\"subject_id\", \"hadm_id\", \"chartdate\", \"hcpcs_cd\", \"seq_num\", \"short_description\"]\n",
    "missing = [c for c in expected_cols if c not in df_hcpcs.columns]\n",
    "if missing:\n",
    "    raise ValueError(f\"⚠️ hcpcsevents CSV is missing required columns: {missing}\")\n",
    "\n",
    "# Keep only expected columns (and in order)\n",
    "df_hcpcs = df_hcpcs[expected_cols]\n",
    "\n",
    "# Parse chartdate to ISO-8601 date (YYYY-MM-DD) text\n",
    "if \"chartdate\" in df_hcpcs.columns:\n",
    "    parsed = pd.to_datetime(df_hcpcs[\"chartdate\"], errors=\"coerce\").dt.strftime(\"%Y-%m-%d\")\n",
    "    df_hcpcs[\"chartdate\"] = parsed\n",
    "\n",
    "# Cast integer-like fields\n",
    "for col in [\"subject_id\", \"hadm_id\", \"seq_num\"]:\n",
    "    df_hcpcs[col] = pd.to_numeric(df_hcpcs[col], errors=\"coerce\").astype(\"Int64\")\n",
    "\n",
    "# Optional: drop duplicates by (subject_id, hadm_id, hcpcs_cd, seq_num)\n",
    "df_hcpcs = df_hcpcs.drop_duplicates(subset=[\"subject_id\", \"hadm_id\", \"hcpcs_cd\", \"seq_num\"])\n",
    "\n",
    "# Insert into SQLite\n",
    "with conn:\n",
    "    df_hcpcs.to_sql(\"hcpcsevents\", conn, if_exists=\"append\", index=False)\n",
    "\n",
    "# Quick verification\n",
    "count = conn.execute(\"SELECT COUNT(*) FROM hcpcsevents;\").fetchone()[0]\n",
    "print(f\"✅ Loaded {count} rows into 'hcpcsevents'.\")\n",
    "display(pd.read_sql(\"SELECT * FROM hcpcsevents LIMIT 5;\", conn))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7634d1ff",
   "metadata": {},
   "source": [
    "### 2.1.13 `Hosp: labevents`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a734d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fb617b52",
   "metadata": {},
   "source": [
    "### 2.1.14 `Hosp: microbiologyevents`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e86a770f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6fde996a",
   "metadata": {},
   "source": [
    "### 2.1.15 `Hosp: patients`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32cff1e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2d60df33",
   "metadata": {},
   "source": [
    "### 2.1.16 `Hosp: pharmacy`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6441e61",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d6ec6aff",
   "metadata": {},
   "source": [
    "### 2.1.17 `Hosp: poe`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b00be8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c90e10ef",
   "metadata": {},
   "source": [
    "### 2.1.18 `Hosp: poe_detail`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd4453f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "58849618",
   "metadata": {},
   "source": [
    "### 2.1.19 `Hosp: prescriptions`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae458d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "79d1178a",
   "metadata": {},
   "source": [
    "### 2.1.20 `Hosp: procedures_icd`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90322b53",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6215a8ea",
   "metadata": {},
   "source": [
    "### 2.1.21 `Hosp: services`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef66959b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5579fdb5",
   "metadata": {},
   "source": [
    "### 2.1.22 `Hosp: transfers`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e23a0008",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "be61154a",
   "metadata": {},
   "source": [
    "# 2.2 `Create Databese: ICU module`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "727a3ed5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "aee5c24f",
   "metadata": {},
   "source": [
    "### 2.2.01 `ICU: `"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c00cfc23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7ad3be2d",
   "metadata": {},
   "source": [
    "### 2.2.02 `ICU: `"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b67caa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7f50249f",
   "metadata": {},
   "source": [
    "### 2.2.03 `ICU: `"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49818877",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ea9bfa",
   "metadata": {
    "vscode": {
     "languageId": "html"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
